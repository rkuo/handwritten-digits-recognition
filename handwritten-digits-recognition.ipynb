{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digits Recognition\n",
    "- Richard Kuo, 05-21-2018\n",
    "\n",
    "## Reference\n",
    "- https://github.com/s4um1l/mnist-tensorflow-simple/blob/master/MNIST%20-%20Simple.ipynb \n",
    "- https://wosaku.github.io/digits-recognition-tensorflow.html\n",
    "- http://dataaspirant.com/2017/05/03/handwritten-digits-recognition-tensorflow-python/\n",
    "- https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd/blob/master/tensorflow-mnist-tutorial/mnist_1.0_softmax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation and Visualization\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MINST data. It is available from the tensorflow package\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# either of following statement works\n",
    "# mnist = input_data.read_data_sets('/tmp/tensorflow/mnist/input_dat', one_hot=True)\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "print(mnist.train.images.shape)\n",
    "print(mnist.test.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first five figures from training set and testing set.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "FLAGS = None\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(mnist.train.images[i].reshape(28, 28))\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(mnist.test.images[i].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label format is the one-hot encoding style. This means that the label corresponds to the index of the array where the value is 1.\n",
    "\n",
    "Example:\n",
    "[1,2,3,4,5,6,7,8,9] \n",
    "[0,0,0,0,0,0,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training labels\")\n",
    "for i in range(5):    \n",
    "    # printout training labels y-value\n",
    "    print(mnist.train.labels[i])\n",
    "\n",
    "print(\"testing labels\")\n",
    "for i in range(5):    \n",
    "    # printout training labels y-value\n",
    "    print(mnist.test.labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "### network structure\n",
    "- 1 input layer with 784 nodes \n",
    "- 1 hidden layer, started with 300 neurons, then try 784 \n",
    "- 1 output layer with 10 nodes\n",
    "- calculation batch size, started with 50 data, then try 100, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters (784-300-10)\n",
    "n_input = 784\n",
    "hidden_layer_neurons = 300\n",
    "n_classes = 10\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.005\n",
    "training_epochs = 30000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x and y placeholders, x is video input, y is the label (actual digits)\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computation model\n",
    "Y = W*X + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights and biases that will be used in the neural network\n",
    "# Todo - hidden layer should be indexed, change x->X later\n",
    "w1 = tf.Variable(tf.random_normal([n_input, hidden_layer_neurons]))\n",
    "w2 = tf.Variable(tf.random_normal([hidden_layer_neurons, n_classes]))\n",
    "b1 = tf.Variable(tf.random_normal([hidden_layer_neurons]))\n",
    "b2 = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The multilayer perceptron model, computation model is defined above\n",
    "# Todo - change the activation function later\n",
    "hidden_layer = tf.nn.sigmoid(tf.add(tf.matmul(x, w1), b1))\n",
    "# aka y-hat/prediction\n",
    "output_layer = tf.add(tf.matmul(hidden_layer, w2), b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function and Optimizer\n",
    "The Cost is defined using the cross-entropy function and Adam optimizer is used to minimize the cost, optional is gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost funcition and optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=y))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# loss/cost\n",
    "# cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_layer))\n",
    "\n",
    "# gradient descent on loss \n",
    "# train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Accuracy\n",
    "- tf.argmax - Returns the index with the largest value across axes of a tensor. \n",
    "- tf.equal - Returns the truth value of (x == y) element-wise.\n",
    "- tf.casts - cast a tensor to a new type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Test model and accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1))\n",
    "correct_prediction = tf.cast(correct_prediction, \"float\")\n",
    "# Computes the mean of elements across dimensions of a tensor.\n",
    "accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start session\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# Accuracies arrays to create a plot\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "epoc_iteration = []\n",
    "\n",
    "# Run the session, save the accuracies\n",
    "for epoch in range(training_epochs):    \n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    if (epoch+1) < 100 or (epoch+1) % 100 == 0:\n",
    "        train_ac = accuracy.eval({x: batch_x, y: batch_y})\n",
    "        validation_ac = accuracy.eval({x: mnist.validation.images, \n",
    "                                       y: mnist.validation.labels})\n",
    "        epoc_iteration.append(epoch+1)\n",
    "        train_accuracies.append(train_ac)\n",
    "        validation_accuracies.append(validation_ac)\n",
    "    sess.run([train_step, cross_entropy], feed_dict={x: batch_x, y: batch_y})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracies\n",
    "# Creates blank canvas\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "axes1 = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
    "\n",
    "# Plot full graph\n",
    "axes1.plot(epoc_iteration, train_accuracies,'-b', label='Training')\n",
    "axes1.plot(epoc_iteration, validation_accuracies,'-g', label='Validation')\n",
    "axes1.legend()\n",
    "axes1.set_xlabel('Epoch')\n",
    "axes1.set_ylabel('Accuracy')\n",
    "axes1.set_title('Training and Validation accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final accuracies\n",
    "print(\"Validation Accuracy:\", accuracy.eval({x: mnist.validation.images, y: mnist.validation.labels}))\n",
    "print(\"Test Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance\n",
    "\n",
    "| Hidden Layer Neurons | Learnling Rate | Epochs | Batch Size | Test Accuracy | Session Timeit | Comments |\n",
    "|:---------------------|:---------------|:-------|:-----------|:--------------|:---------------|:---------|\n",
    "| 300 | 0.005 | 30000 |  50 | 0.8535 | 2m5s | initial, use GradientDescentOptimizer |\n",
    "| 300 | 0.005 | 30000 | 100* | 0.9204 | 2min 34s | fair increase |\n",
    "| 784* | 0.005 | 30000 | 100 | 0.9156 | 4min 58s | time doubled, accuracy reduced a little |\n",
    "| 300 | 0.005 | 30000 | 200* | 0.9193 | 3min 31s | no further improvement |\n",
    "| 300 | 0.005 | 30000 | 100 | 0.9798 | 2min 24s | use AdamOpimizer accurancy increased |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results/model\n",
    "Save W1, W2 and B1 and B2 for future apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weight and bias as theta1 to be used in the proof-of-concept app, theta1 [784,300]\n",
    "theta1 = np.concatenate((b1.eval().reshape(1,hidden_layer_neurons),w1.eval()),axis=0)\n",
    "np.savetxt(\"theta1.csv\", theta1, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weight and bias as theta2 to be used in the proof-of-concept app, theta2 [300, 10]\n",
    "theta2 = np.concatenate((b2.eval().reshape(1,n_classes),w2.eval()),axis=0)\n",
    "np.savetxt(\"theta2.csv\", theta2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
